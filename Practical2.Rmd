---
title: "Kaminda - Practical 2"
output: html_notebook
---


```{r}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
```

```{r}
library(tidyr)
library(ggplot2)
library(scales)
```

```{r}
install.packages("tidytext")
install.packages("textstem")
install.packages("clinspacy")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("stringr")
```

```{r}
library(tidytext)
library(textstem)
library(clinspacy)
library(topicmodels)
library(reshape2)
library(stringr)
```

###Data Parsing

```{r}
raw.data <- clinspacy::dataset_mtsamples()
dplyr::glimpse(raw.data)
```
**1** 

###Data Description

##NoteID - This variable is the unique ID for each note.

##Description - This output provides a summary of the SOAP notes.

##Medical_Specialty - This variable refers to the medical speciality the patient is visiting.

##Sample_Name - This variable refers to the procedures undergone for each patient.

##Transcription - This output provides a full transcript of the physicians SOAP notes.

##Keywords - This output collects keywords from the Medical_Speciality, Sample_Name, and Transcription variables.



```{r rawdata medical specialities}

raw.data %>% dplyr::select(medical_specialty) %>% dplyr::n_distinct()
```

###Transcripts per specialty

```{r}
ggplot2::ggplot(raw.data, ggplot2::aes(y=medical_specialty)) + ggplot2::geom_bar() + labs(x="Document Count", y="Medical Speciality")
```

```{r}
filtered.data <- raw.data %>% dplyr::filter(medical_specialty %in% c("Orthopedic", "Radiology", "Surgery")) 
```


###Text Processing

```{r text processing}

analysis.data <- filtered.data %>%
  unnest_tokens(word, transcription) %>%
  mutate(word = str_replace_all(word, "[^[:alnum:]]", "")) %>%
  filter(!str_detect(word, "[0-9]")) %>%
  anti_join(stop_words) %>%
  group_by(note_id) %>%
  summarise(transcription = paste(word, collapse = " ")) %>%
  left_join(select(filtered.data, -transcription), by = "note_id")
```


```{r}
tokenized.data.unigram <- analysis.data %>% tidytext::unnest_tokens(word, transcription, to_lower=TRUE)
```

```{r}
tokenized.data <- analysis.data %>% tidytext::unnest_tokens(ngram, transcription, token = "ngrams", n=2, to_lower = TRUE)
```

**2**

###Unique Tokens per Speciality

```{r token unigram}
tokenized.data.unigram %>% dplyr::group_by(medical_specialty) %>% dplyr::distinct(word) %>% dplyr::summarise(n=dplyr::n())
```
##Unique Unigrams

##There are 7682 unique unigrams in the orthopedic speciality.

##There are 5935 unique unigrams in the radiology speciality.

##There are 11977 unique unigrams in the surgery speciality.


```{r token bigram}

tokenized.data %>% dplyr::group_by(medical_specialty) %>% dplyr::distinct(ngram) %>% dplyr::summarise(n=dplyr::n())
```


```{r unigram token distribution}

word_counts <- tokenized.data.unigram %>%
    group_by(word) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count))

count_distribution <- word_counts %>%
  group_by(count) %>%
  summarise(num_words = n()) %>%
  ungroup()
 
 ggplot2::ggplot(count_distribution, aes(x = count, y = num_words)) +
  geom_point() +
  labs(title = "Scatter Plot of Count Distribution",
       x = "Count of Unique Words",
       y = "Number of Words")
```

```{r bigram token distribution}
word_counts <- tokenized.data %>%
    group_by(ngram) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count))

count_distribution <- word_counts %>%
  group_by(count) %>%
  summarise(num_words = n()) %>%
  ungroup()
 
 ggplot2::ggplot(count_distribution, aes(x = count, y = num_words)) +
  geom_point() +
  labs(title = "Scatter Plot of Count Distribution",
       x = "Count of Unique Bigrams",
       y = "Number of Words")
```

**3**
###Unique bigrams per category


```{r token bigram2}

tokenized.data %>% dplyr::group_by(medical_specialty) %>% dplyr::distinct(ngram) %>% dplyr::summarise(n=dplyr::n())
```


##Unique Bigrams

##There are 55732 unique bigrams in the orthopedic specialty.

##There are 28297 unique bigrams in the radiology speciality.

##There are 130404 unique bigrams in the surgey speciality.


**4**

##Unique Sentences

```{r}
analysis.data <- filtered.data %>%
  unnest_tokens(sentence, transcription, token = "sentences") %>%
  mutate(sentence = str_replace_all(sentence, "[^[:alnum:]\\s]", "")) %>%
  filter(!str_detect(sentence, "[0-9]")) %>%
  cross_join(stop_words) %>%
  group_by(note_id) %>%
  summarise(transcription = paste(sentence, collapse = " ")) %>%
  left_join(select(filtered.data, -transcription), by = "note_id")
```

```{r}
?cross_join
```

```{r}
?str_detect
```

```{r}
tokenized.data.sentence <- analysis.data %>% tidytext::unnest_tokens(ngram, transcription, token = "sentences", to_lower = TRUE)
```

```{r}
tokenized.data.sentence %>%
  dplyr::group_by(medical_specialty) %>%
  dplyr::count(name = "n") %>%
  dplyr::ungroup()
```

##Unique Sentences

##There are 350 unique bigrams in the orthopedic specialty.

##There are 262 unique bigrams in the radiology speciality.

##There are 1085 unique bigrams in the surgey speciality.

###Words per Category

```{r}
tokenized.data %>%
  dplyr::group_by(medical_specialty) %>%
  dplyr::count(ngram, sort = TRUE) %>%
  dplyr::top_n(5)
```

**5** 

##Use of a Lemmatizer

#A general purpose lemmatizer may not work well for medical data. This is because medical data contains highly specialized terms that require accurately trained methods to be trained to accurately token terms. Some specific issues include:

#a. Medical data usually contains specialized terms, drugs names, and jargon. Therefore, a general purpose tool may not have the knowledge of these terms and may not be proficient in accurately identfying the lemmas.

#b. Medical terms typically come from different parts of speech such as nouns, verbs, and adjectives. Since the process of lemmatizing requires mapping to generate correct lemmas, general purpose lemmas (which have not been trained on medical data) may not process the variations in medical speech effectively.




